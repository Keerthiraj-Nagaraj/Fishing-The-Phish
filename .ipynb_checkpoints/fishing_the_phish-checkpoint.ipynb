{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification metrics and dataset division\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-learn ML models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP \n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering from scikit-learn for text based columns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "filename_1 = \"FIU_Phishing_Mitre_Dataset_split_1.csv\" #Training csv file here\n",
    "filename_2 = \"FIU_Phishing_Mitre_Dataset_split_2.csv\" #Testing csv file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phishing(filename_1, filename_2):\n",
    "\n",
    "    phish_data = pd.read_csv(filename_1)\n",
    "\n",
    "    \n",
    "    #Creating additional features\n",
    "    clean_url = phish_data[\"URL\"]\n",
    "\n",
    "    \n",
    "    #Length of URL\n",
    "    len_url = []\n",
    "\n",
    "    for ur in clean_url:\n",
    "        len_url.append(len(ur))\n",
    "\n",
    "    phish_data['URL_length'] = pd.Series(len_url)\n",
    "\n",
    "    \n",
    "    #Number of slashes in the URL\n",
    "    len_slashes = []\n",
    "\n",
    "    for ur in clean_url:\n",
    "        len_slashes.append(ur.count('/') )\n",
    "\n",
    "    phish_data['URL_slashes'] = pd.Series(len_slashes)\n",
    "\n",
    "    \n",
    "    #Number of dots in the URL\n",
    "    len_dots = []\n",
    "\n",
    "    for ur in clean_url:\n",
    "        len_dots.append(ur.count('.') )\n",
    "\n",
    "    phish_data['URL_dots'] = pd.Series(len_dots)\n",
    "\n",
    "    \n",
    "    #Number of words in the host name of URL\n",
    "    len_host = []\n",
    "\n",
    "    start = '://'\n",
    "    end = '/'\n",
    "\n",
    "    for ur in clean_url:\n",
    "        temp = ur[ur.find(start)+2*len(start) + 2: ur.rfind(end)]\n",
    "        temp = temp.replace('/','.')\n",
    "        temp = temp.replace('-','.')\n",
    "        len_host.append(len(temp.split('.')))\n",
    "\n",
    "    phish_data['URL_host'] = pd.Series(len_host)\n",
    "\n",
    "    total_word_count = 5000\n",
    "    tokenizer = Tokenizer(num_words=total_word_count)\n",
    "    tokenizer.fit_on_texts(clean_url)\n",
    "\n",
    "    seq_length = 5 #Number of items in each sequence\n",
    "    sequences = tokenizer.texts_to_sequences(clean_url)\n",
    "    data = pad_sequences(sequences, maxlen=seq_length)\n",
    "\n",
    "    num_data = phish_data[['create_age(months)', 'expiry_age(months)', 'update_age(days)', 'URL_length', 'URL_slashes', 'URL_dots', 'URL_host']].values\n",
    "    num_lab = phish_data[\"Label\"].values\n",
    "\n",
    "    sscaler = StandardScaler()\n",
    "    num_data_scaled = sscaler.fit_transform(num_data)\n",
    "    num_data = num_data_scaled\n",
    "\n",
    "    random_seed_val = 10\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(num_data_scaled, num_lab, test_size = 0.2, random_state = random_seed_val)\n",
    "\n",
    "    names = [\"Log-Reg\", \"Nearest Neighbors\",\n",
    "             \"Decision Tree\", \"Random Forest\", \"AdaBoost\"]\n",
    "\n",
    "    classifiers = [\n",
    "        LogisticRegression(),\n",
    "        KNeighborsClassifier(5),\n",
    "        DecisionTreeClassifier(max_depth=5),\n",
    "        RandomForestClassifier(max_depth=5, criterion = 'gini', max_features = 'log2', n_estimators = 50),\n",
    "        AdaBoostClassifier()]\n",
    "\n",
    "\n",
    "    y_pred_mat_num = np.zeros((len(X_test), len(names)))\n",
    "    y_pred_mat_num_train = np.zeros((len(X_train), len(names)))\n",
    "\n",
    "    f1_vals = []\n",
    "    f1_vals_train = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    models_1 = []\n",
    "\n",
    "    for name, clf in zip(names, classifiers):\n",
    "\n",
    "        clf.fit(X_train, Y_train)\n",
    "        score = clf.score(X_test, Y_test)\n",
    "        y_hat = clf.predict(X_test)\n",
    "\n",
    "        y_pred_mat_num[:,i] = clf.predict_proba(X_test)[:,1]\n",
    "        y_pred_mat_num_train[:,i] = clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "        f1_vals.append(f1_score(Y_test, y_hat))\n",
    "        f1_vals_train.append(f1_score(Y_train, clf.predict(X_train)))\n",
    "\n",
    "        models_1.append(clf)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    y_num = y_pred_mat_num[:,np.argmax(f1_vals)]\n",
    "    y_num_train = y_pred_mat_num_train[:,np.argmax(f1_vals_train)]\n",
    "\n",
    "    best_model_1 = models_1[np.argmax(f1_vals)]\n",
    "\n",
    "    filename = 'best_model_1.sav'\n",
    "    pickle.dump(best_model_1, open(filename, 'wb'))\n",
    "\n",
    "    ## Text based features with Machine learning models\n",
    "\n",
    "    X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(clean_url, num_lab, test_size = 0.2, random_state = random_seed_val)\n",
    "\n",
    "    y_pred_mat_text = np.zeros((len(X_test_text), len(names)))\n",
    "    y_pred_mat_text_train = np.zeros((len(X_train_text), len(names)))\n",
    "\n",
    "    f1_vals_text = []\n",
    "    f1_vals_text_train = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    models_2 = []\n",
    "\n",
    "    for name, clf in zip(names, classifiers):\n",
    "\n",
    "\n",
    "        classifier = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', clf)])\n",
    "\n",
    "        classifier.fit(X_train_text, y_train_text)\n",
    "        y_hat_text = classifier.predict(X_test_text)\n",
    "\n",
    "        y_pred_mat_text[:,i] = classifier.predict_proba(X_test_text)[:,1]\n",
    "        y_pred_mat_text_train[:,i] = classifier.predict_proba(X_train_text)[:,1]\n",
    "\n",
    "        f1_vals_text.append(f1_score(y_test_text, y_hat_text))\n",
    "        f1_vals_text_train.append(f1_score(y_train_text, classifier.predict(X_train_text)))\n",
    "\n",
    "\n",
    "        models_2.append(classifier)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    y_text = y_pred_mat_text[:,np.argmax(f1_vals_text)]\n",
    "    y_text_train = y_pred_mat_text_train[:,np.argmax(f1_vals_text_train)]\n",
    "\n",
    "\n",
    "    best_model_2 = models_2[np.argmax(f1_vals_text)]\n",
    "\n",
    "    filename = 'best_model_2.sav'\n",
    "    pickle.dump(best_model_2, open(filename, 'wb'))\n",
    "\n",
    "    # Text based features with Deep Learning\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, num_lab, test_size=0.2, random_state=random_seed_val)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_word_count, seq_length, input_length=seq_length))\n",
    "    model.add(LSTM(seq_length, dropout=0.3, recurrent_dropout=0.3))\n",
    "    #model.add(Dense(5, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    ## Fit the model\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "                 ModelCheckpoint(filepath='lstm_url.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "    history = model.fit(x_train, y_train, validation_split=0.1, epochs=10, callbacks = callbacks, verbose = 0)\n",
    "\n",
    "    best_model = load_model('lstm_url.h5')\n",
    "\n",
    "    y_pred_lstm = best_model.predict_classes(x_test)\n",
    "\n",
    "    y_pred_lstm_score = best_model.predict_proba(x_test)\n",
    "    y_pred_lstm_score_train = best_model.predict_proba(x_train)\n",
    "\n",
    "    y_pred_lstm_score = y_pred_lstm_score.reshape(len(y_pred_lstm_score),)\n",
    "    y_pred_lstm_score_train = y_pred_lstm_score_train.reshape(len(y_pred_lstm_score_train),)\n",
    "\n",
    "    fusion = y_num + y_text \n",
    "    #+ y_pred_lstm_score\n",
    "    fusion_train = y_num_train + y_text_train \n",
    "    #+ y_pred_lstm_score_train\n",
    "\n",
    "    f1_fusion_train = []\n",
    "\n",
    "    thr_range = np.arange(0.5, 2, 0.05)\n",
    "\n",
    "    for thr in thr_range:\n",
    "        fusion_pred_train = []\n",
    "\n",
    "        for val in fusion_train:\n",
    "            if val > thr:\n",
    "                fusion_pred_train.append(1)\n",
    "            else:\n",
    "                fusion_pred_train.append(0)\n",
    "\n",
    "\n",
    "        fusion_pred_train = np.array(fusion_pred_train)\n",
    "\n",
    "        f1_fusion_train.append(f1_score(y_train_text, fusion_pred_train))\n",
    "\n",
    "    best_thr = thr_range[np.argmax(f1_fusion_train)]\n",
    "\n",
    "#    print(best_thr)\n",
    "\n",
    "    fusion_pred = []\n",
    "\n",
    "    for val in fusion:\n",
    "        if val > best_thr:\n",
    "            fusion_pred.append(1)\n",
    "        else:\n",
    "            fusion_pred.append(0)\n",
    "\n",
    "    fusion_pred = np.array(fusion_pred)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #Testing phase begins here\n",
    "\n",
    "    phish_data_1 = pd.read_csv(filename_2)\n",
    "\n",
    "    clean_url = phish_data_1[\"URL\"].values #features for model 2\n",
    "\n",
    "    tokenizer.fit_on_texts(clean_url)\n",
    "\n",
    "    seq_length = 5 #Number of items in each sequence\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(clean_url)\n",
    "    data = pad_sequences(sequences, maxlen=seq_length) #features for model 3\n",
    "\n",
    "    len_url = []\n",
    "\n",
    "    for ur in clean_url:\n",
    "        len_url.append(len(ur))\n",
    "\n",
    "    phish_data_1['URL_length'] = pd.Series(len_url)\n",
    "\n",
    "    len_slashes = []\n",
    "\n",
    "    for ur in clean_url:\n",
    "        len_slashes.append(ur.count('/') )\n",
    "\n",
    "    phish_data_1['URL_slashes'] = pd.Series(len_slashes)\n",
    "\n",
    "    len_dots = []\n",
    "\n",
    "    for ur in clean_url:\n",
    "        len_dots.append(ur.count('.') )\n",
    "\n",
    "    phish_data_1['URL_dots'] = pd.Series(len_dots)\n",
    "\n",
    "    len_host = []\n",
    "\n",
    "    start = '://'\n",
    "    end = '/'\n",
    "    #print s[s.find(start)+len(start):s.rfind(end)]\n",
    "\n",
    "    for ur in clean_url:\n",
    "        temp = ur[ur.find(start)+2*len(start) + 2: ur.rfind(end)]\n",
    "        temp = temp.replace('/','.')\n",
    "        temp = temp.replace('-','.')\n",
    "        len_host.append(len(temp.split('.')))\n",
    "\n",
    "    phish_data_1['URL_host'] = pd.Series(len_host)\n",
    "\n",
    "    num_data = phish_data_1[['create_age(months)', 'expiry_age(months)', 'update_age(days)', 'URL_length', 'URL_slashes', 'URL_dots', 'URL_host']].values\n",
    "    num_lab = phish_data_1[\"Label\"].values\n",
    "\n",
    "    num_data_scaled = sscaler.transform(num_data)\n",
    "    num_data = num_data_scaled #features for model 1\n",
    "\n",
    "    loaded_best_model_1 = pickle.load(open('best_model_1.sav', 'rb'))\n",
    "    best_model_1_scores = loaded_best_model_1.predict_proba(num_data)[:,1]\n",
    "\n",
    "    clean_url = phish_data_1[\"URL\"].values\n",
    "\n",
    "    loaded_best_model_2 = pickle.load(open('best_model_2.sav', 'rb'))\n",
    "    best_model_2_scores = loaded_best_model_2.predict_proba(clean_url)[:,1]\n",
    "\n",
    "    loaded_best_model_3 = load_model('lstm_url.h5')\n",
    "    best_model_3_scores = loaded_best_model_3.predict_proba(data)\n",
    "    best_model_3_scores = best_model_3_scores.reshape(len(best_model_3_scores),)\n",
    "\n",
    "    best_fusion = best_model_1_scores + best_model_2_scores \n",
    "    #+ best_model_3_scores\n",
    "\n",
    "    fusion_pred = []\n",
    "\n",
    "    for val in best_fusion:\n",
    "        if val > best_thr:\n",
    "            fusion_pred.append(1)\n",
    "        else:\n",
    "            fusion_pred.append(0)\n",
    "\n",
    "    fusion_pred = np.array(fusion_pred)\n",
    "\n",
    "    print('F1-score from model 1: ')\n",
    "    print(f1_score(num_lab, loaded_best_model_1.predict(num_data)))\n",
    "    print(confusion_matrix(num_lab, loaded_best_model_1.predict(num_data)))\n",
    "    print('')\n",
    "    print('F1-score from model 2: ')\n",
    "    print(f1_score(num_lab, loaded_best_model_2.predict(clean_url)))\n",
    "    print(confusion_matrix(num_lab, loaded_best_model_2.predict(clean_url)))\n",
    "    print('')\n",
    "    print('F1-score from model 3: ')\n",
    "    print(f1_score(num_lab, loaded_best_model_3.predict_classes(data)))\n",
    "    print(confusion_matrix(num_lab, loaded_best_model_3.predict_classes(data)))\n",
    "\n",
    "    print('')\n",
    "    print('F1-score from combined model (model_1+model_2) : ')\n",
    "    print(f1_score(num_lab, fusion_pred))\n",
    "    print(confusion_matrix(num_lab, fusion_pred))\n",
    "\n",
    "    return f1_score(num_lab, fusion_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[231  13]\n",
      " [ 22 233]]\n",
      "F1-score from model 1: \n",
      "0.9087523277467412\n",
      "[[206  38]\n",
      " [ 11 244]]\n",
      "\n",
      "F1-score from model 2: \n",
      "0.9008264462809917\n",
      "[[233  11]\n",
      " [ 37 218]]\n",
      "\n",
      "F1-score from model 3: \n",
      "0.8486055776892432\n",
      "[[210  34]\n",
      " [ 42 213]]\n",
      "\n",
      "F1-score from combined model (model_1+model_2: \n",
      "0.9301397205588823\n",
      "[[231  13]\n",
      " [ 22 233]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9301397205588823"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phishing(filename_1, filename_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
