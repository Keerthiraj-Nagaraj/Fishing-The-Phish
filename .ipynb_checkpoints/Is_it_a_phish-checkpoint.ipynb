{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is it a Phish?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation and intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "sns.set()\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification metrics and dataset division\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Scikit-learn ML models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NLP \n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Feature engineering from scikit-learn for text based columns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(filename):\n",
    "    \n",
    "    #reading csv file    \n",
    "    phish_data = pd.read_csv(filename)\n",
    "\n",
    "    #Extracting url data\n",
    "    clean_url = phish_data[\"URL\"]\n",
    "\n",
    "    #Addtional URL based features\n",
    "\n",
    "    #1. Length of url\n",
    "\n",
    "    len_url = []\n",
    "\n",
    "    for ur in clean_url:\n",
    "        len_url.append(len(ur))\n",
    "\n",
    "    phish_data['URL_length'] = pd.Series(len_url)\n",
    "\n",
    "    #2. Number of slashes\n",
    "\n",
    "    len_slashes = []\n",
    "\n",
    "    for ur in clean_url:\n",
    "        len_slashes.append(ur.count('/') )\n",
    "\n",
    "    phish_data['URL_slashes'] = pd.Series(len_slashes)\n",
    "\n",
    "    #3 Number of dots\n",
    "\n",
    "\n",
    "    len_dots = []\n",
    "\n",
    "    for ur in clean_url:\n",
    "        len_dots.append(ur.count('.') )\n",
    "\n",
    "    phish_data['URL_dots'] = pd.Series(len_dots)\n",
    "\n",
    "\n",
    "    len_host = []\n",
    "\n",
    "    start = '://'\n",
    "    end = '/'\n",
    "\n",
    "    for ur in clean_url:\n",
    "        temp = ur[ur.find(start)+2*len(start) + 2: ur.rfind(end)]\n",
    "        temp = temp.replace('/','.')\n",
    "        temp = temp.replace('-','.')\n",
    "        len_host.append(len(temp.split('.')))\n",
    "\n",
    "    phish_data['URL_host'] = pd.Series(len_host)\n",
    "    \n",
    "    print('Feature engineering completed')    \n",
    "    \n",
    "    #Numerical features for ML models\n",
    "\n",
    "    num_data = phish_data[['create_age(months)', 'expiry_age(months)', 'update_age(days)', 'URL_length', 'URL_slashes', 'URL_dots', 'URL_host']].values\n",
    "    num_lab = phish_data[\"Label\"].values\n",
    "\n",
    "    #Scaling input features\n",
    "    sscaler = StandardScaler()\n",
    "    num_data_scaled = sscaler.fit_transform(num_data)\n",
    "    num_data = num_data_scaled\n",
    "\n",
    "    #random seed\n",
    "    random_seed_val = random.randint(0,50)\n",
    "    \n",
    "    print('random seed value: ', random_seed_val)\n",
    "\n",
    "    #Defining ML models \n",
    "    names = [\"Log-Reg\", \"Nearest Neighbors\",\n",
    "             \"Decision Tree\", \"Random Forest\", \"AdaBoost\"]\n",
    "\n",
    "    classifiers = [\n",
    "        LogisticRegression(),\n",
    "        KNeighborsClassifier(5),\n",
    "        DecisionTreeClassifier(max_depth=5),\n",
    "        RandomForestClassifier(max_depth=5, criterion = 'gini', max_features = 'log2', n_estimators = 50),\n",
    "        AdaBoostClassifier()]\n",
    "    \n",
    "    #Model 1\n",
    "    \n",
    "    #Train Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(num_data_scaled, num_lab, test_size = 0.2, random_state = random_seed_val)\n",
    "\n",
    "    # Variables to save predicted probabilities\n",
    "    y_pred_mat_num = np.zeros((len(X_test), len(names)))\n",
    "    y_pred_mat_num_train = np.zeros((len(X_train), len(names)))\n",
    "\n",
    "    #Saving F1-scores for training and test datastes\n",
    "    f1_vals = []\n",
    "    f1_vals_train = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    models_1 = []\n",
    "\n",
    "    for name, clf in zip(names, classifiers):\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y_hat = clf.predict(X_test)\n",
    "\n",
    "        y_pred_mat_num[:,i] = clf.predict_proba(X_test)[:,1]\n",
    "        y_pred_mat_num_train[:,i] = clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "        f1_vals.append(f1_score(y_test, y_hat))\n",
    "        f1_vals_train.append(f1_score(y_train, clf.predict(X_train)))\n",
    "\n",
    "        models_1.append(clf)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    #Saving the best model 1 based on F1-scores\n",
    "\n",
    "    y_num = y_pred_mat_num[:,np.argmax(f1_vals)]\n",
    "    y_num_train = y_pred_mat_num_train[:,np.argmax(f1_vals_train)]\n",
    "    best_model_1 = models_1[np.argmax(f1_vals)]\n",
    "\n",
    "    print('model 1 training performance')\n",
    "    print(np.argmax(f1_vals_train))\n",
    "    print(np.max(f1_vals))\n",
    "    \n",
    "    model_name_1 = 'best_model_1.sav'\n",
    "    pickle.dump(best_model_1, open(model_name_1, 'wb'))\n",
    "    print('Model 1 saved')\n",
    "\n",
    "    # Model 2 - TF-IDF with original url and ML models\n",
    "\n",
    "    X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(clean_url, num_lab, test_size = 0.2, random_state = random_seed_val)\n",
    "\n",
    "    # Variables to save predict probabilities for model 2\n",
    "\n",
    "    y_pred_mat_text = np.zeros((len(X_test_text), len(names)))\n",
    "    y_pred_mat_text_train = np.zeros((len(X_train_text), len(names)))\n",
    "\n",
    "\n",
    "    # Saving F1-scores for model 2\n",
    "    f1_vals_text = []\n",
    "    f1_vals_text_train = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    models_2 = []\n",
    "\n",
    "    for name, clf in zip(names, classifiers):\n",
    "\n",
    "        #NLP pipeline with vectorization and TFIDF\n",
    "        classifier = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', clf)])\n",
    "\n",
    "        classifier.fit(X_train_text, y_train_text)\n",
    "        y_hat_text = classifier.predict(X_test_text)\n",
    "\n",
    "        y_pred_mat_text[:,i] = classifier.predict_proba(X_test_text)[:,1]\n",
    "        y_pred_mat_text_train[:,i] = classifier.predict_proba(X_train_text)[:,1]\n",
    "\n",
    "        f1_vals_text.append(f1_score(y_test_text, y_hat_text))\n",
    "        f1_vals_text_train.append(f1_score(y_train_text, classifier.predict(X_train_text)))\n",
    "        models_2.append(classifier)\n",
    "        i += 1\n",
    "\n",
    "    #Saving best pipeline parameters(model) for model 2\n",
    "    y_text = y_pred_mat_text[:,np.argmax(f1_vals_text)]\n",
    "    y_text_train = y_pred_mat_text_train[:,np.argmax(f1_vals_text_train)]\n",
    "    best_model_2 = models_2[np.argmax(f1_vals_text)]\n",
    "\n",
    "    model_name_2 = 'best_model_2.sav'\n",
    "    pickle.dump(best_model_2, open(model_name_2, 'wb'))\n",
    "    print('Model 2 saved')\n",
    "    \n",
    "    \n",
    "    # Model - 3\n",
    "\n",
    "    #BOW features from URL for LSTM (deep learning model) with Keras\n",
    "    total_word_count = 5000\n",
    "    tokenizer = Tokenizer(num_words=total_word_count)\n",
    "    tokenizer.fit_on_texts(clean_url)\n",
    "    seq_length = 5 #Number of items in each sequence\n",
    "    sequences = tokenizer.texts_to_sequences(clean_url)\n",
    "    data = pad_sequences(sequences, maxlen=seq_length)\n",
    "\n",
    "    #Scaling the features\n",
    "    sscaler = StandardScaler()\n",
    "    num_data_scaled = sscaler.fit_transform(num_data)\n",
    "    num_data = num_data_scaled\n",
    "\n",
    "    #training and testing sets for LSTM model\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, num_lab, test_size=0.2, random_state=random_seed_val)\n",
    "\n",
    "    #Defining Keras model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_word_count, seq_length, input_length=seq_length))\n",
    "    model.add(LSTM(seq_length, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model_name_3 = 'lstm_url.h5'\n",
    "    ## Fitting the LSTM model\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "                 ModelCheckpoint(filepath= model_name_3, monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "    model.fit(x_train, y_train, validation_split=0.1, epochs=10, callbacks = callbacks, verbose = 0)\n",
    "    \n",
    "    print('Model 3 saved')\n",
    "    \n",
    "    best_model = load_model('lstm_url.h5')\n",
    "\n",
    "    #Predicting classification probabilities for LSTM model\n",
    "    y_pred_lstm = best_model.predict_classes(x_test)\n",
    "    y_pred_lstm_score = best_model.predict_proba(x_test)\n",
    "    y_pred_lstm_score_train = best_model.predict_proba(x_train)\n",
    "\n",
    "    #Reshaping to combine with probability scores from other models\n",
    "    y_pred_lstm_score = y_pred_lstm_score.reshape(len(y_pred_lstm_score),)\n",
    "    y_pred_lstm_score_train = y_pred_lstm_score_train.reshape(len(y_pred_lstm_score_train),)\n",
    "\n",
    "    #Combining prediction probabilities\n",
    "\n",
    "    #simple addition\n",
    "    fusion = y_num + y_text + y_pred_lstm_score\n",
    "    fusion_train = y_num_train + y_text_train + y_pred_lstm_score_train\n",
    "\n",
    "    #Experiments for choosing the threshold\n",
    "    f1_fusion_train = []\n",
    "    thr_range = np.arange(0.5, 3, 0.05)\n",
    "\n",
    "    for thr in thr_range:\n",
    "        fusion_pred_train = []\n",
    "\n",
    "        for val in fusion_train:\n",
    "            if val > thr:\n",
    "                fusion_pred_train.append(1)\n",
    "            else:\n",
    "                fusion_pred_train.append(0)\n",
    "\n",
    "\n",
    "        fusion_pred_train = np.array(fusion_pred_train)\n",
    "\n",
    "        f1_fusion_train.append(f1_score(y_train_text, fusion_pred_train))\n",
    "\n",
    "    best_thr = thr_range[np.argmax(f1_fusion_train)]\n",
    "    \n",
    "    #Returns best_threshold, names of model files to load during testing\n",
    "    \n",
    "    print('Combined model training complete')\n",
    "    \n",
    "    \n",
    "    return best_thr, model_name_1, model_name_2, model_name_3, sscaler, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(train_file_name, test_file_name):\n",
    "    \n",
    "    #read csv files  \n",
    "\n",
    "    train_data = pd.read_csv(train_file_name)\n",
    "    phish_data_1 = pd.read_csv(test_file_name)\n",
    "\n",
    "    best_thr, model_name_1, model_name_2, model_name_3, sscaler, tokenizer = train_model(train_file_name)\n",
    "    \n",
    "    print('trained models and necessary parameters for testing obtained')\n",
    "    \n",
    "    \n",
    "    #getting url data\n",
    "    clean_url = phish_data_1[\"URL\"]\n",
    "\n",
    "    #Feature preparation for test data\n",
    "    len_url = []\n",
    "    for ur in clean_url:\n",
    "        len_url.append(len(ur))\n",
    "\n",
    "    phish_data_1['URL_length'] = pd.Series(len_url)\n",
    "\n",
    "\n",
    "    len_slashes = []\n",
    "    for ur in clean_url:\n",
    "        len_slashes.append(ur.count('/') )\n",
    "\n",
    "    phish_data_1['URL_slashes'] = pd.Series(len_slashes)\n",
    "\n",
    "\n",
    "    len_dots = []\n",
    "    for ur in clean_url:\n",
    "        len_dots.append(ur.count('.') )\n",
    "\n",
    "    phish_data_1['URL_dots'] = pd.Series(len_dots)\n",
    "\n",
    "\n",
    "    len_host = []\n",
    "    start = '://'\n",
    "    end = '/'\n",
    "\n",
    "    for ur in clean_url:\n",
    "        temp = ur[ur.find(start)+2*len(start) + 2: ur.rfind(end)]\n",
    "        temp = temp.replace('/','.')\n",
    "        temp = temp.replace('-','.')\n",
    "        len_host.append(len(temp.split('.')))\n",
    "\n",
    "    phish_data_1['URL_host'] = pd.Series(len_host)\n",
    "\n",
    "    #Extracting numerical features for model\n",
    "    num_data = phish_data_1[['create_age(months)', 'expiry_age(months)', 'update_age(days)', 'URL_length', 'URL_slashes', 'URL_dots', 'URL_host']].values\n",
    "    num_lab = phish_data_1[\"Label\"].values\n",
    "    \n",
    "    #Scaling the numerical data with model fitted on training data\n",
    "    num_data_scaled = sscaler.transform(num_data)\n",
    "    num_data = num_data_scaled #features for model 1\n",
    "\n",
    "\n",
    "    #Tokenizing URL data with model fitted on training data\n",
    "    tokenizer.fit_on_texts(clean_url)\n",
    "    seq_length = 5 #Number of items in each sequence\n",
    "    sequences = tokenizer.texts_to_sequences(clean_url)\n",
    "    data = pad_sequences(sequences, maxlen=seq_length) #features for model 3\n",
    "\n",
    "\n",
    "    #Loading models\n",
    "    print('Loading model 1')\n",
    "    loaded_best_model_1 = pickle.load(open(model_name_1, 'rb'))\n",
    "    best_model_1_scores = loaded_best_model_1.predict_proba(num_data)[:,1]\n",
    "\n",
    "    print('Loading model 2')\n",
    "    loaded_best_model_2 = pickle.load(open(model_name_2, 'rb'))\n",
    "    best_model_2_scores = loaded_best_model_2.predict_proba(clean_url)[:,1]\n",
    "\n",
    "    print('Loading model 3')\n",
    "    loaded_best_model_3 = load_model(model_name_3)\n",
    "    best_model_3_scores = loaded_best_model_3.predict_proba(data)\n",
    "    best_model_3_scores = best_model_3_scores.reshape(len(best_model_3_scores),)\n",
    "\n",
    "    best_fusion = best_model_1_scores + best_model_2_scores + best_model_3_scores\n",
    "\n",
    "\n",
    "    fusion_pred = []\n",
    "\n",
    "    for val in best_fusion:\n",
    "        if val > best_thr:\n",
    "            fusion_pred.append(1)\n",
    "        else:\n",
    "            fusion_pred.append(0)\n",
    "\n",
    "    fusion_pred = np.array(fusion_pred)\n",
    "    \n",
    "    print('Printing performance metrics - F1-score and confusion matrices')\n",
    "    \n",
    "    print('F1-score and confusion matrix from model 1: ')\n",
    "    print(f1_score(num_lab, loaded_best_model_1.predict(num_data)))\n",
    "    print(confusion_matrix(num_lab, loaded_best_model_1.predict(num_data)))\n",
    "    print('')\n",
    "\n",
    "    print('F1-score and confusion matrix from model 2: ')\n",
    "    print(f1_score(num_lab, loaded_best_model_2.predict(clean_url)))\n",
    "    print(confusion_matrix(num_lab, loaded_best_model_2.predict(clean_url)))\n",
    "    print('')\n",
    "\n",
    "    print('F1-score and confusion matrix from model 3: ')\n",
    "    print(f1_score(num_lab, loaded_best_model_3.predict_classes(data)))\n",
    "    print(confusion_matrix(num_lab, loaded_best_model_3.predict_classes(data)))\n",
    "    print('')\n",
    "\n",
    "    print('F1-score and confusion matrix from combined model: ')\n",
    "    print(f1_score(num_lab, fusion_pred))\n",
    "    print(confusion_matrix(num_lab, fusion_pred))\n",
    "    print('')\n",
    "    \n",
    "    final_f1_score = f1_score(num_lab, fusion_pred)\n",
    "    \n",
    "    print('Combined model F1-score returned')\n",
    "    \n",
    "    return final_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'FIU_Phishing_Mitre_Dataset_split_1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'FIU_Phishing_Mitre_Dataset_split_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed\n",
      "random seed value:  38\n",
      "model 1 training performance\n",
      "1\n",
      "0.9215686274509803\n",
      "Model 1 saved\n",
      "Model 2 saved\n",
      "Train on 3096 samples, validate on 344 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6828 - acc: 0.6437 - val_loss: 0.6541 - val_acc: 0.8808\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.5638 - acc: 0.8346 - val_loss: 0.4474 - val_acc: 0.8401\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.4020 - acc: 0.8521 - val_loss: 0.3440 - val_acc: 0.8779\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.3310 - acc: 0.8753 - val_loss: 0.2978 - val_acc: 0.8837\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.2621 - acc: 0.9076 - val_loss: 0.2600 - val_acc: 0.8953\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.2166 - acc: 0.9276 - val_loss: 0.2349 - val_acc: 0.9128\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.1772 - acc: 0.9448 - val_loss: 0.2162 - val_acc: 0.9215\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.1435 - acc: 0.9622 - val_loss: 0.2025 - val_acc: 0.9273\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.1173 - acc: 0.9713 - val_loss: 0.1946 - val_acc: 0.9302\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0963 - acc: 0.9777 - val_loss: 0.1929 - val_acc: 0.9273\n",
      "Model 3 saved\n",
      "Combined model training complete\n",
      "trained models and necessary parameters for testing obtained\n",
      "Loading model 1\n",
      "Loading model 2\n",
      "Loading model 3\n",
      "Printing performance metrics - F1-score and confusion matrices\n",
      "F1-score and confusion matrix from model 1: \n",
      "0.5503685503685504\n",
      "[[204  40]\n",
      " [143 112]]\n",
      "\n",
      "F1-score and confusion matrix from model 2: \n",
      "0.8981288981288981\n",
      "[[234  10]\n",
      " [ 39 216]]\n",
      "\n",
      "F1-score and confusion matrix from model 3: \n",
      "0.8492063492063491\n",
      "[[209  35]\n",
      " [ 41 214]]\n",
      "\n",
      "F1-score and confusion matrix from combined model: \n",
      "0.8674698795180723\n",
      "[[217  27]\n",
      " [ 39 216]]\n",
      "\n",
      "Combined model F1-score returned\n"
     ]
    }
   ],
   "source": [
    "final_score = test_model(train_file_name=train_file, test_file_name=test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
